#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""aws-deepgram-2026-03 - Pipecat Voice Agent

This bot uses a cascade pipeline: Speech-to-Text → LLM → Text-to-Speech

Generated by Pipecat CLI

Required AI services:
- Deepgram (Speech-to-Text)
- Aws_Bedrock (LLM)
- Deepgram (Text-to-Speech)

Run the bot using::

    uv run bot.py
"""


from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair, LLMUserAggregatorParams
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from dotenv import load_dotenv
from pipecat.pipeline.runner import PipelineRunner
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.runner.types import RunnerArguments
import os
from pipecat.transports.base_transport import BaseTransport
from pipecat.runner.types import DailyRunnerArguments
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.services.deepgram.tts import DeepgramTTSService
from pipecat.pipeline.pipeline import Pipeline
from pipecat.transports.daily.transport import DailyTransport, DailyParams
from pipecat.services.aws.llm import AWSBedrockLLMService
from pipecat.frames.frames import LLMRunFrame
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.services.llm_service import FunctionCallParams

load_dotenv(override=True)


# Define a function using the standard schema
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get the current weather in a location",
    properties={
        "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA",
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "The temperature unit to use.",
        },
    },
    required=["location", "format"]
)

# Main function handler - called to execute the function
async def fetch_weather_from_api(params: FunctionCallParams):
    # Fetch weather data from your API
    weather_data = {"conditions": "sunny", "temperature": "75"}
    await params.result_callback(weather_data)




async def run_bot(transport: BaseTransport):
    """Main bot logic."""
    logger.info("Starting bot")

    # Speech-to-Text service
    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    # Text-to-Speech service
    tts = DeepgramTTSService(
            api_key=os.getenv("DEEPGRAM_API_KEY"),
            # voice=os.getenv("DEEPGRAM_VOICE_ID")
        )


    # LLM service
    llm = AWSBedrockLLMService(
        aws_region=os.getenv("AWS_REGION"),
        model=os.getenv("AWS_BEDROCK_MODEL"),
        params=AWSBedrockLLMService.InputParams(temperature=0.8)
    )

    # Register the weather function
    llm.register_function(
        "get_current_weather",
        fetch_weather_from_api,
        cancel_on_interruption=True,  # Cancel if user interrupts (default: True)
    )



    messages = [
        {
            "role": "user",
            "content": "You are a friendly AI assistant. Respond naturally and keep your answers conversational, and VERY BRIEF. Your output is being converted to audio, so don't include any emoji or special characters.",
        },
    ]

    tools = ToolsSchema(standard_tools=[weather_function])

    context = LLMContext(messages=messages, tools=tools )
    user_aggregator, assistant_aggregator = LLMContextAggregatorPair(
        context,
        user_params=LLMUserAggregatorParams(
            vad_analyzer=SileroVADAnalyzer(),
        ),
    )


    


    # Pipeline - assembled from reusable components
    pipeline = Pipeline([
        transport.input(),

        stt,

        user_aggregator,

        llm,

        tts,

        
        transport.output(),

        
        assistant_aggregator,

    ])


    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[
        ],
    )

    @task.rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()




    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point."""
    # Krisp is available when deployed to Pipecat Cloud
    if os.environ.get("ENV") != "local":
        from pipecat.audio.filters.krisp_viva_filter import KrispVivaFilter

        krisp_filter = KrispVivaFilter()
    else:
        krisp_filter = None

    transport = None

    match runner_args:
        case DailyRunnerArguments():
            transport = DailyTransport(
                runner_args.room_url,
                runner_args.token,
                "Pipecat Bot",
                params=DailyParams(
                    audio_in_enabled=True,
                    audio_in_filter=krisp_filter,
                    audio_out_enabled=True,
                ),
            )
        case _:
            logger.error(f"Unsupported runner arguments type: {type(runner_args)}")
            return

    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()
